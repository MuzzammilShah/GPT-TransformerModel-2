{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "!!! note \"How to read?\"\n",
    "    - Section Headers (In Bold) to segregate the main categories.\n",
    "    - Sub Headers to represent the respective section explained by sensei in the video\n",
    "    - The code snippet breakdown: Till that point along with explaination on what was done. The final code snippet is in the `Train.py` file."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Introduction**\n",
    "\n",
    "----------\n",
    "\n",
    "We will be reproducing the GPT 2 model that was released by OpenAI based on their paper and the source code released. We will be working on the 124 million parameter model, which was the smallest of the mini-series which was released- So, during each release, mini models are made i.e. from smaller parameters to larger ones. And usually the larger ones end up being THE \"GPT Model\".\n",
    "\n",
    "The source code of GPT 2 provided by OpenAI was implemented in TensorFlow, but we will be implementing it in PyTorch.\n",
    "\n",
    "We can even load this model from the HuggingFace library as then we can even access all the parameter value settings that was provided to that original 124M model.\n",
    "\n",
    "Now, the original implementation code was very complex and hard to understand, so we will be doing our own implementation and building it from scratch to reproduce it. But what our first step will be, is to load the original 124M model from HuggingFace itself into OUR CLASS, therefore we are importing all of the properties, especially the weights and the parameters. So we are ensuring we are within the same environment as the original code but will be doing our own implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F\n",
    "\n",
    "#=========================================================\n",
    "\n",
    "@dataclass\n",
    "class GPTConfig:\n",
    "    block_size: int = 256\n",
    "    vocab_size: int = 85\n",
    "    n_layer: int = 6\n",
    "    n_head: int = 6\n",
    "    n_embd: int = 384\n",
    "\n",
    "class GPT(nn.Module):\n",
    "    def __init__(self, config):\n",
    "        super().__init__()\n",
    "        self.config = config"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Section 1**\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start with, in the GPT 2 paper they have made slight adjustments to the [original transformer model](https://arxiv.org/pdf/1706.03762) implementation (As seen in the below image), i.e. The Encoder section and The Cross-Attention block which actually utilises the encoder section, itself are completely removed. Therefore GPT Architecture is known as a Decoder only architecture model.\n",
    "\n",
    "![GPT Architecture](assets/gpt-architecture.png)\n",
    "\n",
    "Everything else will remain the same, but there will be some differences that we will implement. \n",
    "\n",
    "In the [GPT 2 paper](https://cdn.openai.com/better-language-models/language_models_are_unsupervised_multitask_learners.pdf), in `page 4` under `section 2.3 Model` they have mentioned *\"Layer normalization (Ba et al., 2016) was moved to the input of each sub-block, similar to a pre-activation residual network (He et al., 2016) and an additional layer normalization was added after the final selfattention block.\"*.\n",
    "\n",
    "So basically there have been some reshuffling of the order of the layers and the addition of a layer which are:\n",
    "\n",
    "- The Norm Layer (layer norm - ln) is added before the Multi-Head attention layer\n",
    "- One more Norm Layer (layer norm - ln) has been added before the final section of the model i.e. after the self-attention block and before Linear-Softmax layers.\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Implementing the GPT-2 nn.Module\n",
    "\n",
    "Now, we will be implementing our `nn.Modules` and we will be using the schema reference of the GPT 2 model which we loaded from HuggingFace in **section 0**, which were:\n",
    "\n",
    "![GPT 2 Schema](assets/gpt-2-schema.png)\n",
    "\n",
    "So our aim would be to match up/replicate the above schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "self.transformer = nn.ModuleDict(dict(\n",
    "    wte = nn.Embedding(config.vocab_size, config.n_embd),\n",
    "    wpe = nn.Embedding(config.block_size, config.n_emb),\n",
    "    h = nn.ModuleList([Block(config) for _ in range(config.n_layer)]),\n",
    "    ln_f = nn.LayerNorm(config.n_emb)\n",
    "))\n",
    "self.lm_head = nn.Linear(config.vocab_size, config.n_embd, bias=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`self.transformer = nn.ModuleDict(dict())`\n",
    "\n",
    "- In the above schema image we see that the main container which contains all the modules in called 'transformer', therefore that is what we have declared first.\n",
    "- We are then reflecting the `transformer` module using `nn.ModuleDict` which basically allows you to index into the sub-modules using keys, just like in a dictionary. Our keys are basically strings.\n",
    "\n",
    "Then within that `transformer` module we have-\n",
    "\n",
    "`wte = nn.Embedding(config.vocab_size, config.n_embd)` and `wpe = nn.Embedding(config.block_size, config.n_emb)`\n",
    "\n",
    "- which are the tensor and positional embeddings respectively.\n",
    "- both of these modules are `nn.Embedding` modules, and a `nn.Embedding` module is just a \"fancy wrapper module\" for a single array/list/block of numbers, so they just a single tensor.\n",
    "- so `nn.Embedding` is just a glorified wrapper around these tensor that allows you to access its elements by indexing into their rows.\n",
    "\n",
    "`h = nn.ModuleList([Block(config) for _ in range(config.n_layer) ])`\n",
    "\n",
    "- in the schema you can see that `h` is being declared, but the indexing is happening through an integer value i.e. from 0 to 11 (unlike the other modules where indexing was through a string).\n",
    "- therefore we declare it as a List `nn.ModuleList` so that we can index it using integers exactly as we see in the schema.\n",
    "- now the `h` module, the module list has a `n_layer` `Blocks`, the `Blocks` still need to be defined (we will in a while).\n",
    "- the `h` probably stands for 'hidden'\n",
    "\n",
    "`ln_f = nn.LayerNorm(config.n_embd)`\n",
    "\n",
    "- this is based on us following the GPT 2 paper where we have to define the additional 'Final Layer Norm', so thats what we have done.\n",
    "\n",
    "So that is the end of the Transformer Module. After that, we have the final Classifier, which is-\n",
    "\n",
    "`self.lm_head = nn.Linear(config.vocab_size, config.n_embd, bias=False)`\n",
    "\n",
    "- The final classifier, which is the Language Model Head (lm_head) which projects the number of embeddings (n_embd, which is 786 in the image) all the way to the vocab size (vocab_size, which is 50257 in the image) and GPT 2 uses no bias for this final projection.\n",
    "\n",
    "**Therefore this is the skeleton structure of what we saw in the architecture diagram! Below is a breakdown of it for a clearer understanding:**\n",
    "\n",
    "![GPT 2 Skeleton structure breakdown](assets/gpt-2-replicated-part1.png)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
