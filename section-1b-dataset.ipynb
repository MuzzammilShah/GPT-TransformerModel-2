{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0e6ef66c",
   "metadata": {},
   "source": [
    "### Initial loading and viewing the dataset\n",
    "\n",
    "Here we will be seeing the dataset that we are going to use, as well as the intuition of all the operations we will be performing on the main code. This is just a forked branch from the main path in section 1b."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "37c4ba3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "M r. and Mrs. Dursley, of number four, Privet Drive, were proud to say that they were perfectly norm\n"
     ]
    }
   ],
   "source": [
    "with open('cleaned_dataset.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "data = text[:1000]\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3b036a",
   "metadata": {},
   "source": [
    "- We are loading the dataset and reading it into *text*. We are then storing the first 1000 words into *data* and then printing just the first 100 from *data*.\n",
    "- We can also note that the tokenizer of gpt2 **roughly has a compression rate of about 3:1**, so the 1000 words stored will roughly have 300 tokens that will come out of *data*.\n",
    "\n",
    "If we want some additional stats on the dataset, we can run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2b065f71",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Lines: 79295\n",
      "Words: 1095233\n",
      "Bytes: 6199345\n"
     ]
    }
   ],
   "source": [
    "def wc_equivalent(filename):\n",
    "    with open(filename, 'rb') as f:  # open in binary mode to get bytes accurately\n",
    "        content = f.read()\n",
    "        byte_count = len(content)\n",
    "\n",
    "    text = content.decode('utf-8', errors='ignore')  # decode bytes to string\n",
    "    lines = text.splitlines()\n",
    "    line_count = len(lines)\n",
    "    word_count = len(text.split())\n",
    "\n",
    "    print(f\"Lines: {line_count}\")\n",
    "    print(f\"Words: {word_count}\")\n",
    "    print(f\"Bytes: {byte_count}\")\n",
    "\n",
    "# Usage\n",
    "wc_equivalent('cleaned_dataset.txt')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5c4ff88",
   "metadata": {},
   "source": [
    "Sensei just used `wc input.txt` on the terminal and he got the similar set of variables as ouput. I have no idea how he did that as `wc` wasnt recognised in mine (maybe a pip install for it? idk, as of this moment i have no idea lol), so i just pplxed it and got that code snippet.\n",
    "\n",
    "Now lets go ahead and tokenize the *data*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d3995296",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[44, 374, 13, 290, 9074, 13, 360, 1834, 1636, 11, 286, 1271, 1440, 11, 4389, 16809, 9974, 11, 547, 6613, 284, 910, 326, 484]\n"
     ]
    }
   ],
   "source": [
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "tokens = enc.encode(data)\n",
    "print(tokens[:24])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8fbcb96",
   "metadata": {},
   "source": [
    "So we used OpenAI's tiktoken for our tokenization, we asked for the gpt2 encoding and then encoded the *data* and finally printing the first 24 (If you check in the tiktokenizer app we will get 27 tokens in total for our first 100 words).\n",
    "\n",
    "Fun fact: Turns out in tiktokens, the newline slash is represented by the token number `198`, in our case we haven't encountered yet, but just wanted to mention it to check in the future."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd3b28a3",
   "metadata": {},
   "source": [
    "Now, we have got this one dimensional value of tokens from our data. We now want to be able to feed this into our neural network so they can be processed.\n",
    "\n",
    "So in our case, we need to feed these tokens into the indeces value `idx` of the `forward()` method of our GPT class, therefore we need them in the shape of *(B, T)* where T is the maximum sequence length that can be passed.\n",
    "\n",
    "We will now see how we can convert that 1D value of tokens into this 2D size so that it can be passed into the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3d93c1d7",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   44,   374,    13,   290,  9074,    13],\n",
      "        [  360,  1834,  1636,    11,   286,  1271],\n",
      "        [ 1440,    11,  4389, 16809,  9974,    11],\n",
      "        [  547,  6613,   284,   910,   326,   484]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "buf = torch.tensor(tokens[:24])\n",
    "x = buf.view(4, 6)\n",
    "print(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "df3a8f01",
   "metadata": {},
   "source": [
    "So above cell is one such method which sensei likes to implement to achieve the inputs we want to feed into the model.\n",
    "\n",
    "We `import torch` and create a tensor object which contains the first 24 tokens, but we are rearranging them using `view()` to be in a 2D array. Here ofcourse 4, 6 is because we have only chosen 24 tokens.\n",
    "\n",
    "Now, those are the values that are passed into `forward()`, so if we take an example:\n",
    "\n",
    "- if *13* in the `idx`, then we know the layer will only consider the previous tokens till there *44,   374,   13* and will use those to predict the next value which is *290*. \n",
    "- So each token has a target which it needs to predict.\n",
    "\n",
    "Just for this 24 tokens, you can see that the last token doesnot have that \"next token\" to be predicted, so we are just writing this additional lines of code such that we can have the next target token also in the object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "4c6e7502",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([[   44,   374,    13,   290,  9074,    13],\n",
      "        [  360,  1834,  1636,    11,   286,  1271],\n",
      "        [ 1440,    11,  4389, 16809,  9974,    11],\n",
      "        [  547,  6613,   284,   910,   326,   484]])\n",
      "tensor([[  374,    13,   290,  9074,    13,   360],\n",
      "        [ 1834,  1636,    11,   286,  1271,  1440],\n",
      "        [   11,  4389, 16809,  9974,    11,   547],\n",
      "        [ 6613,   284,   910,   326,   484,   547]])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "buf = torch.tensor(tokens[:24 + 1])\n",
    "x = buf[:-1].view(4, 6)\n",
    "y = buf[1:].view(4, 6)\n",
    "print(x)\n",
    "print(y)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30d47eac",
   "metadata": {},
   "source": [
    "\n",
    "-----\n",
    "\n",
    "So ultimately, this is what he likes to usually do:\n",
    "- Load all the tokens\n",
    "- Convert them into dimensions of *(B, T)*\n",
    "- Load them into two types of tensor objects: (i) is what we feed into the transformer and (ii) contains the labels of what it needs to predict next, so ultimately we are passing *(B, T, T+1)*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "586aa0ba",
   "metadata": {},
   "source": [
    "Now, you can go back to the main notebook path in section-1b **Let’s train: data batches (B,T) → logits (B,T,C)** to continue!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
