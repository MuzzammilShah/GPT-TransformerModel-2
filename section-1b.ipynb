{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d23ccf2",
   "metadata": {},
   "source": [
    "# **Section 1 - B**\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d0bf65",
   "metadata": {},
   "source": [
    "## Sample, auto-detect the device\n",
    "\n",
    "At the end of the previous section, we had loaded the pretrained gpt2 model and its weights into our architecture and generated the model. But now we could like to initialize our own weights, we want the model to weights to be generated randomly.\n",
    "\n",
    "So that can be done fairly simple way:\n",
    "\n",
    "```\n",
    "#model = GPT.from_pretrained(\"gpt2\")\n",
    "model = GPT(GPTConfig())\n",
    "```\n",
    "\n",
    "we just call our default `GPTConfig()` that we made. So what PyTorch does is that, it internally assigns random weights to each of the layers in our config, therefore we can use this to generate text from our model.\n",
    "\n",
    "Lastly, before i run this, we also added an additional line of code to better control the device used to run this model. In my case i do have a GPU with CUDA capability. So, if you want to run the model until this point you can also do that using CPU. We have added this additional flag point just to show which device you are using here:\n",
    "\n",
    "```\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "print(f\"Device used: {device})\n",
    "```\n",
    "\n",
    "The rest of the code follows a dynamic approach of detecting the device as well (even in the `forward()` you will see that we have used `device=idx.device`), therefore we are ensuring that all the layers are using the same device while generating.\n",
    "\n",
    "And this is the final output that we generated!\n",
    "\n",
    "![Sampling and Auto detect output](assets/auto-device-output.png)\n",
    "\n",
    "Obviously it is gibberish lol, we will get to the training next!\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa40857",
   "metadata": {},
   "source": [
    "## Let’s train: data batches (B,T) → logits (B,T,C)\n",
    "\n",
    "> **NOTE**\n",
    ">\n",
    ">We will be loading our dataset now. Sensei used his fav \"The tiny Shakespear dataset\", I am going ahead and using MY Favourite dataset which is what i also used for my GPT-1 implementation which is the HARRY POTTER NOVELS COLLECTION dataset. I directly took the `cleaned_dataset.txt` file which i had processed.\n",
    ">\n",
    ">If you want to see a simple breakdown version of the dataset and what we are about to do, take a look at [this notebook](https://github.com/MuzzammilShah/GPT-TransformerModel-2/blob/main/section-1b-dataset.ipynb) on my repo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64a764a3",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "print(f\"Device used: {device}\")\n",
    "\n",
    "num_return_sequences = 5\n",
    "max_length = 30\n",
    "\n",
    "#==========THIS SECTION==========\n",
    "import tiktoken\n",
    "enc = tiktoken.get_encoding('gpt2')\n",
    "\n",
    "with open('cleaned_dataset.txt', 'r') as f:\n",
    "    text = f.read()\n",
    "\n",
    "data = text[:1000]\n",
    "tokens = enc.encode(data)\n",
    "\n",
    "B, T = 4, 32\n",
    "buf = torch.tensor(tokens[:B*T + 1])\n",
    "x = buf[:-1].view(B, T)\n",
    "y = buf[1:].view(B, T)\n",
    "#================================\n",
    "\n",
    "#model = GPT.from_pretrained(\"gpt2\")\n",
    "model = GPT(GPTConfig())\n",
    "model.eval()\n",
    "model.to(device)\n",
    "\n",
    "#==========THIS SECTION==========\n",
    "x = x.to(device)\n",
    "logits = model(x)\n",
    "print(logits.shape)\n",
    "import sys; sys.exit(0)\n",
    "#================================"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7c631d1",
   "metadata": {},
   "source": [
    "So the above SECTIONS are the newly added codes just like how they were done in the [dataset breakdown notebook](https://github.com/MuzzammilShah/GPT-TransformerModel-2/blob/main/section-1b-dataset.ipynb), we are only performing a debugging step here therefore the values have been hardcoded. Since we have a batch of 4 by 32, we get the logits for that.\n",
    "\n",
    "- The output we got when the program was run (notice there is a sys exit): **`torch.Size([4, 32, 50257])`**\n",
    "\n",
    "- So `50257` are the logits for what comes next at every position. That is the `x`.\n",
    "\n",
    "-----\n",
    "\n",
    "### **Debugging moment, yay! (Mini version)**\n",
    "\n",
    "So, i had encountered the error `RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu!`.\n",
    "\n",
    "This wouldn't happen in sensei's video as he had a manual OVERIDE of the device to cpu. In my case i am continuing to stick with cuda. But since our buffer was initialised manually here: `buf = torch.tensor(tokens[:B*T + 1])`, it by default sits in the cpu. \n",
    "\n",
    "To fix this, we just added one additional line of code: `x = x.to(device)` just before calculating the logits.\n",
    "\n",
    "-----\n",
    "\n",
    "Next we still have the `y` which contains the targets. So now is the time to calculate the loss -> do the backward pass -> and do the optimization. Lets go ahead and calculate the loss first.\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35d1cbfc",
   "metadata": {},
   "source": [
    "## Cross entropy loss"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
