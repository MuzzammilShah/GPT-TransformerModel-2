{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5d23ccf2",
   "metadata": {},
   "source": [
    "# **Section 1 - B**\n",
    "\n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "62d0bf65",
   "metadata": {},
   "source": [
    "## Sample, auto-detect the device\n",
    "\n",
    "At the end of the previous section, we had loaded the pretrained gpt2 model and its weights into our architecture and generated the model. But now we could like to initialize our own weights, we want the model to weights to be generated randomly.\n",
    "\n",
    "So that can be done fairly simple way:\n",
    "\n",
    "```\n",
    "#model = GPT.from_pretrained(\"gpt2\")\n",
    "model = GPT(GPTConfig())\n",
    "```\n",
    "\n",
    "we just call our default `GPTConfig()` that we made. So what PyTorch does is that, it internally assigns random weights to each of the layers in our config, therefore we can use this to generate text from our model.\n",
    "\n",
    "Lastly, before i run this, we also added an additional line of code to better control the device used to run this model. In my case i do have a GPU with CUDA capability. So, if you want to run the model until this point you can also do that using CPU. We have added this additional flag point just to show which device you are using here:\n",
    "\n",
    "```\n",
    "device = \"cpu\"\n",
    "if torch.cuda.is_available():\n",
    "    device = \"cuda\"\n",
    "\n",
    "print(f\"Device used: {device})\n",
    "```\n",
    "\n",
    "The rest of the code follows a dynamic approach of detecting the device as well (even in the `forward()` you will see that we have used `device=idx.device`), therefore we are ensuring that all the layers are using the same device while generating.\n",
    "\n",
    "And this is the final output that we generated!\n",
    "\n",
    "![Sampling and Auto detect output](assets/auto-device-output.png)\n",
    "\n",
    "Obviously it is gibberish lol, we will get to the training next!\n",
    "\n",
    "&nbsp;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "baa40857",
   "metadata": {},
   "source": [
    "## Let’s train: data batches (B,T) → logits (B,T,C)\n",
    "\n",
    "> **NOTE**\n",
    ">\n",
    ">We will be loading our dataset now. Sensei used his fav \"The tiny Shakespear dataset\", I am going ahead and using MY Favourite dataset which is what i also used for my GPT-1 implementation which is the HARRY POTTER NOVELS COLLECTION dataset. I directly took the `cleaned_dataset.txt` file which i had processed.\n",
    ">\n",
    ">If you want to see a simple breakdown version of the dataset and what we are about to do, take a look at [this notebook](https://github.com/MuzzammilShah/GPT-TransformerModel-2/blob/main/section-1b-dataset.ipynb) on my repo."
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
